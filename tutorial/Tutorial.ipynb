{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174aa05f",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This tutorial assumes that you are completely new to NASLib, and will introduce you to the core ideas and APIs of the library. By the end of it, you will know everything you need to know to make a submission to the [Zero-Cost NAS Competition](https://codalab.lisn.upsaclay.fr/competitions/3932)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c78d4",
   "metadata": {},
   "source": [
    "## Introduction to Search Spaces\n",
    "\n",
    "Let's begin by taking a look at the [NAS-Bench-201](https://arxiv.org/abs/2001.00326) search space as an example:\n",
    "\n",
    "<img src=\"./images/nb201_arch.png\" alt=\"alt text\" title=\"image Title\" width=\"700\"/>\n",
    "\n",
    "As you can see, the architecture consists of multiple *cells* stacked together with residual blocks in between which downsample the feature maps. Each cell has 6 edges, each of which could hold one of 5 operations from a predefined operation set.\n",
    "\n",
    "Let's first create a search space in NASLib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf26de8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph makrograph-0.1983816, scope None, 20 nodes"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from naslib.search_spaces import NasBench201SearchSpace\n",
    "search_space = NasBench201SearchSpace(n_classes=10)\n",
    "search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722635fc",
   "metadata": {},
   "source": [
    "Equivalently, you could also create the search space as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75996eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph makrograph-0.6036780, scope None, 20 nodes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from naslib.search_spaces import get_search_space\n",
    "search_space = get_search_space(name='nasbench201', dataset='cifar10')\n",
    "search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c093ae5",
   "metadata": {},
   "source": [
    "`Graphs` in NASLib inherit from both [PyTorch Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) as well as [NetworkX DiGraph](https://networkx.org/documentation/stable/reference/classes/digraph.html). It uses [NetworkX](https://networkx.org/) to create the architecture structure, which can later be parsed to create a standard PyTorch Module.\n",
    "\n",
    "Here's a closer look at the search_space graph that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8781f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in the graph: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Edges in the graph: [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20)]\n"
     ]
    }
   ],
   "source": [
    "print('Nodes in the graph:', search_space.nodes())\n",
    "print('Edges in the graph:', search_space.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baabb241",
   "metadata": {},
   "source": [
    "Every node and edge of the `Graph` can hold operations, which could be either a concrete implementation of the NASLib `AbstractPrimitive`, or another `Graph`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0306eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation on edge 1-2 of the graph: Stem(\n",
      "  (seq): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Operation on edge 2-3 of the graph: Graph named 'cell' with 4 nodes and 6 edges\n"
     ]
    }
   ],
   "source": [
    "print('Operation on edge 1-2 of the graph:', search_space.edges[1, 2]['op']) # Concrete implementation of AbstractPrimitive as 'op' on the edge\n",
    "print('Operation on edge 2-3 of the graph:', search_space.edges[2, 3]['op']) # NASLib Graph as 'op' on the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d071e04",
   "metadata": {},
   "source": [
    "Here's a closer look at a cell, which is itself another `Graph`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b90506b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in the cell: [1, 2, 3, 4]\n",
      "Edges in the cell: [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "cell = search_space.edges[2, 3]['op']\n",
    "print('Nodes in the cell:', cell.nodes())\n",
    "print('Edges in the cell:', cell.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f1118",
   "metadata": {},
   "source": [
    "The cell graph thus has the same structure as the cell shown in the NAS-Bench-201 architecture figure. Let's now look at what is present on the edges of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db74b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations on edge 1-2 of cell:\n",
      "[Identity(), Zero (stride=1), ReLUConvBN(\n",
      "  (op): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "), ReLUConvBN(\n",
      "  (op): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "), AvgPool1x1(\n",
      "  (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(f'Operations on edge 1-2 of cell:')\n",
    "print(cell.edges[1, 2]['op'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811af732",
   "metadata": {},
   "source": [
    "All edges on the cell have a list of candidate operations on them, as seen above. The candidate operations here are Identity, Zero, ReLU-3x3Convolution-BatchNorm, ReLU-1x1Convolution-BatchNorm, and a 1x1AveragePool. You can uncomment and run the next lines to see all the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffe636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for edge in cell.edges:\n",
    "#     print('%'*50)\n",
    "#     print(f'Operations on edge 1-2 of cell:')\n",
    "#     print(cell.edges[edge]['op'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab170f",
   "metadata": {},
   "source": [
    "## Sampling random models\n",
    "\n",
    "Let's now sample a random architecture from the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43e541f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before sampling operation\n",
      "Operation on edge 1-2: [Identity(), Zero (stride=1), ReLUConvBN(\n",
      "  (op): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "), ReLUConvBN(\n",
      "  (op): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  )\n",
      "), AvgPool1x1(\n",
      "  (avgpool): AvgPool2d(kernel_size=3, stride=1, padding=1)\n",
      ")]\n",
      "\n",
      "After sampling operation\n",
      "Operation on edge (1, 2): Identity()\n",
      "Operation on edge (1, 3): ReLUConvBN(\n",
      "  (op): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Operation on edge (1, 4): Zero (stride=1)\n",
      "Operation on edge (2, 3): Zero (stride=1)\n",
      "Operation on edge (2, 4): ReLUConvBN(\n",
      "  (op): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Operation on edge (3, 4): Identity()\n",
      "\n",
      "Architecture encoding: (0, 2, 1, 1, 2, 0)\n"
     ]
    }
   ],
   "source": [
    "# A model can be sampled from a search space Graph only once\n",
    "# So clone the search space first\n",
    "graph = search_space.clone()\n",
    "cell = graph.edges[2, 3]['op']\n",
    "\n",
    "# Initially, the operation on an edge is simply a list of all candidate operations\n",
    "print('Before sampling operation')\n",
    "print('Operation on edge 1-2:', cell.edges[1, 2]['op'])\n",
    "\n",
    "# Sample a random architecture\n",
    "graph.sample_random_architecture()\n",
    "\n",
    "# After sampling, it is replaced by one operation from the list\n",
    "# This means you cannot invoke sample_random_architecture()\n",
    "# on the same graph twice\n",
    "print('\\nAfter sampling operation')\n",
    "for edge in cell.edges():\n",
    "    print(f'Operation on edge {edge}:', cell.edges[edge]['op'])\n",
    "\n",
    "# Get the representation of this architecture:\n",
    "print('\\nArchitecture encoding:', graph.get_hash())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5c2f7",
   "metadata": {},
   "source": [
    "To create the PyTorch model, one needs only to parse this `Graph` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10785e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub modules in the graph before parsing []\n",
      "\n",
      "Sub modules in the graph after parsing [Stem(\n",
      "  (seq): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "), Graph cell-0.5767110, scope stage_1, 4 nodes, Graph cell-0.4516561, scope stage_1, 4 nodes, Graph cell-0.7715191, scope stage_1, 4 nodes, Graph cell-0.8664710, scope stage_1, 4 nodes, Graph cell-0.1674664, scope stage_1, 4 nodes, ResNetBasicblock(\n",
      "  (conv_a): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv_b): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (downsample): Sequential(\n",
      "    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "), Graph cell-0.2991547, scope stage_2, 4 nodes, Graph cell-0.9848268, scope stage_2, 4 nodes, Graph cell-0.4333180, scope stage_2, 4 nodes, Graph cell-0.8190599, scope stage_2, 4 nodes, Graph cell-0.0986330, scope stage_2, 4 nodes, ResNetBasicblock(\n",
      "  (conv_a): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv_b): ReLUConvBN(\n",
      "    (op): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (downsample): Sequential(\n",
      "    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "), Graph cell-0.7796141, scope stage_3, 4 nodes, Graph cell-0.6157184, scope stage_3, 4 nodes, Graph cell-0.0597902, scope stage_3, 4 nodes, Graph cell-0.6364680, scope stage_3, 4 nodes, Graph cell-0.9865471, scope stage_3, 4 nodes, Sequential(\n",
      "  (op): Sequential(\n",
      "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): AdaptiveAvgPool2d(output_size=1)\n",
      "    (3): Flatten(start_dim=1, end_dim=-1)\n",
      "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")]\n",
      "\n",
      "Result: tensor([[ 0.2043,  0.1416, -0.4398, -0.2264, -0.2273,  0.3394,  0.2632, -0.2477,\n",
      "          0.2011, -0.0592]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create the graph\n",
    "graph = search_space.clone()\n",
    "graph.sample_random_architecture()\n",
    "\n",
    "# Parse the NASLib graph and make it a PyTorch model\n",
    "print('Sub modules in the graph before parsing', list(graph.children()))\n",
    "graph.parse()\n",
    "print('\\nSub modules in the graph after parsing', list(graph.children()))\n",
    "\n",
    "# Test the graph with a forward pass of a small random minibatch\n",
    "result = graph(torch.randn(1, 3, 32, 32))\n",
    "print('\\nResult:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf594896",
   "metadata": {},
   "source": [
    "## Querying the performance of a model\n",
    "\n",
    "NAS-Bench-201 has 15,625 models in its search space, all of which have been evaluated for classification task on three separate datasets - CIFAR10, CIFAR100, and ImageNet-16-120. Given the architecture encoding of a model, one can simply query the benchmark to see its final performance for any one of these tasks.\n",
    "\n",
    "The first thing to do is to ensure that you have the benchmark data files. For now, we will download and test only the NAS-Bench-201 benchmark for the CIFAR10 task. \n",
    "\n",
    "If you've already downloaded the data and tested the API, you can skip this step. If these scripts do not work for you, please follow the instructions [here](https://github.com/automl/NASLib/tree/automl-conf-competition/dataset_preparation#-dataset-preparation) to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1ac43c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for NASBench201 API\n",
      "dataset = cifar10\n",
      "search_space = nb201\n",
      "cifar10 exists\n",
      "Testing NASBench201 API\n",
      "Testing (search_space, task) api for (nasbench201, cifar10)... Success\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ../scripts/bash_scripts/download_data.sh\n",
    "\n",
    "!echo Downloading data for NASBench201 API\n",
    "!cd .. && source scripts/bash_scripts/download_data.sh nb201 cifar10\n",
    "\n",
    "!echo Testing NASBench201 API\n",
    "!cd .. && python test_benchmark_apis.py --search_space nasbench201 --task cifar10 --show_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0e1535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function to sample a new model from the given search space and parse it\n",
    "def sample_and_parse_graph(name='nasbench201', dataset='cifar10'):\n",
    "    search_space = get_search_space(name, dataset)\n",
    "    search_space.sample_random_architecture()\n",
    "    search_space.parse()\n",
    "    return search_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a819fd6",
   "metadata": {},
   "source": [
    "Now, lets sample a model from the search space and query its validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9272b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compact model representation is: (2, 0, 0, 3, 3, 2)\n",
      "NAS-Bench-201 representation is: |nor_conv_3x3~0|+|skip_connect~0|nor_conv_1x1~1|+|skip_connect~0|nor_conv_1x1~1|nor_conv_3x3~2|\n",
      "Validation accuracy: 90.62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, load the benchmark API.\n",
    "from naslib.utils import get_dataset_api\n",
    "dataset_api = get_dataset_api('nasbench201', 'cifar10')\n",
    "\n",
    "# Sample a random architecture model\n",
    "graph = sample_and_parse_graph()\n",
    "\n",
    "# Show architecture encoding\n",
    "from naslib.search_spaces.nasbench201.conversions import convert_naslib_to_str\n",
    "\n",
    "print(f'Compact model representation is: {graph.get_hash()}')\n",
    "print(f'NAS-Bench-201 representation is: {convert_naslib_to_str(graph)}')\n",
    "\n",
    "# Query the benchmark\n",
    "from naslib.search_spaces.core.query_metrics import Metric\n",
    "val_accuracy = graph.query(\n",
    "    metric=Metric.VAL_ACCURACY,\n",
    "    dataset='cifar10',\n",
    "    dataset_api=dataset_api\n",
    ")\n",
    "\n",
    "print(f'Validation accuracy: {val_accuracy}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9d44c",
   "metadata": {},
   "source": [
    "## Zero Cost Predictors\n",
    "Let's now move on to trying the zero cost predictors already available in NASLib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e2841c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 10 models\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Scoring models with predictor\n",
      "Querying benchmarks for actual scores\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluates a ZeroCost predictor for a search space and dataset/task\"\"\"\n",
    "from naslib.predictors import ZeroCost\n",
    "from naslib.utils import utils\n",
    "import numpy as np\n",
    "\n",
    "# Get the configs from naslib/configs/predictor_config.yaml (and the command line arguments, if any)\n",
    "# The configs include the zero-cost method to use, the search space and dataset/task to use, amongst others.\n",
    "# For now, we will manually update the config here\n",
    "config = utils.get_config_from_args()\n",
    "# print(config)\n",
    "config.search_space='nasbench201'\n",
    "config.dataset='cifar10'\n",
    "\n",
    "# Initialize the predictor\n",
    "# Method type can be \"fisher\", \"grasp\", \"grad_norm\", \"jacov\", \"snip\", \"synflow\", \"flops\" or \"params\"\n",
    "predictor = ZeroCost(method_type=config.predictor)\n",
    "\n",
    "# Create the models to score\n",
    "n = 10\n",
    "print(f'Sampling {n} models')\n",
    "models = [sample_and_parse_graph() for i in range(n)]\n",
    "\n",
    "# Get the dataloader for this dataset\n",
    "train_loader, val_loader, test_loader, train_transform, val_transform = utils.get_train_val_loaders(config=config)\n",
    "\n",
    "# Score each model\n",
    "print('Scoring models with predictor')\n",
    "scores = [predictor.query(model, dataloader=test_loader) for model in models]\n",
    "\n",
    "# Query benchmarks to get the actual scores\n",
    "print('Querying benchmarks for actual scores')\n",
    "actual_scores = [\n",
    "        model.query(\n",
    "            metric=Metric.VAL_ACCURACY,\n",
    "            dataset='cifar10',\n",
    "            dataset_api=dataset_api\n",
    "        ) for model in models\n",
    "    ]\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56cf4f",
   "metadata": {},
   "source": [
    "The Kendall Tau correlation of the predicted and actual scores is the metric of interest in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ba8a742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KendalltauResult(correlation=0.5555555555555555, pvalue=0.02860945767195767)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.kendalltau(scores, actual_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c18c14",
   "metadata": {},
   "source": [
    "To make the evaluation of predictors more convenient, `ZeroCostPredictorEvaluator` class is provided to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17d2ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/11 09:48:49 nl.evaluators.zc_evaluator]: \u001b[0mSampling from search space...\n",
      "\u001b[32m[04/11 09:48:52 nl.evaluators.zc_evaluator]: \u001b[0mQuerying the predictor\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[04/11 09:49:02 nl.evaluators.zc_evaluator]: \u001b[0mCompute evaluation metrics\n",
      "\u001b[32m[04/11 09:49:02 nl.evaluators.zc_evaluator]: \u001b[0mdataset: cifar10, predictor: synflow, kendalltau 0.5604\n",
      "\u001b[32m[04/11 09:49:02 nl.evaluators.zc_evaluator]: \u001b[0mmae: 2.7995851399501432e+40, rmse: 1.0437419559137192e+41, pearson: 0.2368, spearman: 0.7451, kendalltau: 0.5604, kt_2dec: 0.5604, kt_1dec: 0.5604, precision_10: 0.9, precision_20: 0.6, full_ytest: [86.77 71.92 85.18 89.47 87.4  84.85 85.19 89.01 85.17 89.89 87.12 84.12\n",
      " 89.44 85.69], full_testpred: [2.10175186e+19 2.66518826e+06 2.94391780e+25 2.12203568e+35\n",
      " 1.63192861e+31 2.52076873e+32 2.08993446e+23 1.95421532e+26\n",
      " 5.09783416e+15 1.41177900e+39 1.96238489e+19 2.56321128e+06\n",
      " 3.90529928e+41 1.55693468e+18], query_time: 0.7541, \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5604395604395604"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from naslib.evaluators.zc_evaluator import ZeroCostPredictorEvaluator\n",
    "from naslib.utils import setup_logger\n",
    "import logging\n",
    "\n",
    "# Set up logger\n",
    "logger = setup_logger(config.save + \"/log.log\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Change the default test_size in configs\n",
    "config.test_size = 14\n",
    "\n",
    "# Initialize the ZeroCostPredictorEvaluator class\n",
    "predictor_evaluator = ZeroCostPredictorEvaluator(predictor, config=config)\n",
    "predictor_evaluator.adapt_search_space(search_space, dataset_api=dataset_api)\n",
    "\n",
    "# Evaluate the predictor\n",
    "results = predictor_evaluator.evaluate()\n",
    "results[-1]['kendalltau']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf21cae",
   "metadata": {},
   "source": [
    "## Sample Submission\n",
    "\n",
    "Now, you're ready to create a sample submission for the competition. In this example, we're going to write a very simple zero-cost predictor, which simply counts the number of parameters in the model. The predictor thus assigns higher score to bigger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e646e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from naslib.search_spaces.core.graph import Graph\n",
    "from naslib.predictors.predictor import Predictor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "class ZeroCostPredictor(Predictor):\n",
    "    \"\"\" A sample submission class.\n",
    "\n",
    "    CodaLab will import this class from your submission.py, and evaluate it.\n",
    "    Your class must be named `ZeroCostPredictor`, and must contain self.method_type\n",
    "    with your method name in it.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.method_type = 'MyZeroCostPredictorName'\n",
    "\n",
    "    def pre_process(self) -> None:\n",
    "        \"\"\" This method is called exactly once before query is called repeatedly with the models to score \"\"\"\n",
    "        pass\n",
    "\n",
    "    def query(self, graph: Graph, dataloader:DataLoader=None) -> float:\n",
    "        \"\"\" Predict the score of the given model\n",
    "\n",
    "        Args:\n",
    "            graph       : Model to score\n",
    "            dataloader  : DataLoader for the task to predict. E.g., if the task is to\n",
    "                          predict the score of a model for classification on CIFAR10 dataset,\n",
    "                          a CIFAR10 Dataloader is passed here.\n",
    "\n",
    "        Returns:\n",
    "            Score of the model. Higher the score, higher the model is ranked.\n",
    "        \"\"\"\n",
    "\n",
    "        # You can consume the dataloader and pass the data through the model here.\n",
    "        # Uncomment the following lines to try this\n",
    "\n",
    "        # data, labels = next(iter(dataloader))\n",
    "        # logits = graph(data)\n",
    "\n",
    "        # In this example, however, we simply count the number of parameters in the model\n",
    "        # This zero-cost predictor thus gives higher score to larger models.\n",
    "        score = count_parameters(graph)\n",
    "\n",
    "        # Higher the score, higher the ranking of the model\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac499e",
   "metadata": {},
   "source": [
    "You can evaluate the performance of this predictor across all the benchmarks (search-space/dataset combination) available in NASLib as follows. \n",
    "\n",
    "*Make sure you set up the data for NAS-Bench-201 with CIFAR100 and ImageNet16-120 before you run the next cell. Manual download instructions [here](https://github.com/automl/NASLib/tree/automl-conf-competition/dataset_preparation#-dataset-preparation)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98f97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/11 09:49:02 nl.utils.utils]: \u001b[0mCommand line args: Namespace(config_file=None, opts=[], datapath=None)\n",
      "\u001b[32m[04/11 09:49:02 nl.utils.utils]: \u001b[0mExperiment dir : run/cifar10/predictors/synflow/1000\n",
      "\u001b[32m[04/11 09:49:02 nl.utils.utils]: \u001b[0mExperiment dir : run/cifar10/predictors/synflow/1000/search\n",
      "\u001b[32m[04/11 09:49:02 nl.utils.utils]: \u001b[0mExperiment dir : run/cifar10/predictors/synflow/1000/eval\n",
      "\u001b[32m[04/11 09:49:03 nl.evaluators.zc_evaluator]: \u001b[0mSampling from search space...\n",
      "\u001b[32m[04/11 09:49:05 nl.evaluators.zc_evaluator]: \u001b[0mQuerying the predictor\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[04/11 09:49:09 nl.evaluators.zc_evaluator]: \u001b[0mCompute evaluation metrics\n",
      "\u001b[32m[04/11 09:49:09 nl.evaluators.zc_evaluator]: \u001b[0mdataset: cifar10, predictor: MyZeroCostPredictorName, kendalltau 0.4243\n",
      "\u001b[32m[04/11 09:49:09 nl.evaluators.zc_evaluator]: \u001b[0mmae: 425572.515, rmse: 518474.3217, pearson: 0.5307, spearman: 0.5124, kendalltau: 0.4243, kt_2dec: 0.4243, kt_1dec: 0.4243, precision_10: 0.9, precision_20: 0.45, full_ytest: [86.77 71.92 85.18 89.47 87.4  84.85 85.19 89.01 85.17 89.89], full_testpred: [ 129306  101306  559386  559386  587386  587386  129306  400346  129306\n",
      " 1073466], query_time: 0.3104, \n",
      "\u001b[32m[04/11 09:49:10 nl.evaluators.zc_evaluator]: \u001b[0mSampling from search space...\n",
      "\u001b[32m[04/11 09:49:12 nl.evaluators.zc_evaluator]: \u001b[0mQuerying the predictor\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\u001b[32m[04/11 09:49:16 nl.evaluators.zc_evaluator]: \u001b[0mCompute evaluation metrics\n",
      "\u001b[32m[04/11 09:49:16 nl.evaluators.zc_evaluator]: \u001b[0mdataset: cifar100, predictor: MyZeroCostPredictorName, kendalltau 0.4243\n",
      "\u001b[32m[04/11 09:49:16 nl.evaluators.zc_evaluator]: \u001b[0mmae: 431444.4, rmse: 523304.2823, pearson: 0.5585, spearman: 0.4939, kendalltau: 0.4243, kt_2dec: 0.4243, kt_1dec: 0.4243, precision_10: 0.9, precision_20: 0.45, full_ytest: [65.32 46.62 62.5  69.1  64.4  63.04 62.52 68.52 63.36 70.62], full_testpred: [ 135156  107156  565236  565236  593236  593236  135156  406196  135156\n",
      " 1079316], query_time: 0.3593, \n",
      "\u001b[32m[04/11 09:49:17 nl.evaluators.zc_evaluator]: \u001b[0mSampling from search space...\n",
      "\u001b[32m[04/11 09:49:19 nl.evaluators.zc_evaluator]: \u001b[0mQuerying the predictor\n",
      "\u001b[32m[04/11 09:49:26 nl.evaluators.zc_evaluator]: \u001b[0mCompute evaluation metrics\n",
      "\u001b[32m[04/11 09:49:26 nl.evaluators.zc_evaluator]: \u001b[0mdataset: ImageNet16-120, predictor: MyZeroCostPredictorName, kendalltau 0.5185\n",
      "\u001b[32m[04/11 09:49:26 nl.evaluators.zc_evaluator]: \u001b[0mmae: 432772.44, rmse: 524398.9466, pearson: 0.6353, spearman: 0.673, kendalltau: 0.5185, kt_2dec: 0.5185, kt_1dec: 0.5185, precision_10: 0.9, precision_20: 0.45, full_ytest: [35.9333 14.9333 35.4    40.7333 39.4333 34.8333 33.9    42.1667 33.5667\n",
      " 44.7   ], full_testpred: [ 136456  108456  566536  566536  594536  594536  136456  407496  136456\n",
      " 1080616], query_time: 0.7046, \n",
      "nasbench201              ||cifar10                  ||0.4242640687119285\n",
      "nasbench201              ||cifar100                 ||0.4242640687119285\n",
      "nasbench201              ||ImageNet16-120           ||0.5185449728701348\n",
      "Average Kendall-Tau: 0.4556910367646639\n"
     ]
    }
   ],
   "source": [
    "# This function evaluates a predictor with test_size number of models across the benchmarks available for \n",
    "# the given search space\n",
    "from naslib.evaluators import full_evaluate_predictor\n",
    "\n",
    "predictor = ZeroCostPredictor()\n",
    "\n",
    "# You can also test the zero cost predictors already available in NASLib\n",
    "# from naslib.predictors import ZeroCost\n",
    "# predictor = ZeroCost(method_type='synflow')\n",
    "\n",
    "all_benchmarks = {\n",
    "    'nasbench201': ['cifar10', 'cifar100', 'ImageNet16-120'],\n",
    "    'nasbench301': ['cifar10'],\n",
    "    'transbench101_micro': ['class_scene', 'class_object', 'jigsaw'],\n",
    "}\n",
    "\n",
    "\n",
    "# search_spaces = [\"nasbench201\", \"nasbench301\", \"transbench101_micro\"]\n",
    "full_evaluate_predictor(predictor, test_size=10, search_spaces=[\"nasbench201\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e88d7",
   "metadata": {},
   "source": [
    "For the submission to be complete, the class `ZeroCostPredictor` must be saved in a file named `submission.py` and zipped together with an empty `metdata` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2a4893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: sample_submission/metadata (stored 0%)\r\n",
      "  adding: sample_submission/submission.py (deflated 57%)\r\n"
     ]
    }
   ],
   "source": [
    "# Directory structure:\n",
    "# sample_submission/\n",
    "# |__ metadata        # empty file\n",
    "# |__ submission.py   # File with zero-cost predictor code\n",
    "!touch sample_submission/metadata\n",
    "!zip sample_submission.zip sample_submission/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25395cf6",
   "metadata": {},
   "source": [
    "You can use this `sample_submission.zip` as a test submission on CodaLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f109db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
